import * as utils from '@contentlayer/utils'
import type { MkdirError } from '@contentlayer/utils/node'
import { fileOrDirExists } from '@contentlayer/utils/node'
import { pipe } from '@effect-ts/core'
import * as Chunk from '@effect-ts/core/Collections/Immutable/Chunk'
import * as T from '@effect-ts/core/Effect'
import * as S from '@effect-ts/core/Effect/Experimental/Stream'
import * as OT from '@effect-ts/otel'
import { camelCase } from 'camel-case'
import { promises as fs, watch } from 'fs'
import * as path from 'path'
import type { Observable } from 'rxjs'
import { combineLatest, defer, of } from 'rxjs'
import { switchMap, tap } from 'rxjs/operators'
import type { PackageJson } from 'type-fest'

import type { Cache } from '../cache'
import type { PluginOptions, SourcePlugin, SourcePluginType } from '../plugin'
import type { DocumentTypeDef, SchemaDef } from '../schema'
import { makeArtifactsDir, makeArtifactsDirEff } from '../utils'
import { autogeneratedNote } from './common'
import { renderTypes } from './generate-types'

/**
 * Used to track which files already have been written.
 * Gets re-initialized per `generateDotpkg` invocation therefore only "works" during dev mode.
 */
type FilePath = string
type DocumentHash = string
type WrittenFilesCache = Record<FilePath, DocumentHash>

export type GenerationOptions = {
  sourcePluginType: SourcePluginType
  options: PluginOptions
}

// TODO make sure unused old generated files are removed
export const generateDotpkg = ({ source, watchData }: { source: SourcePlugin; watchData: boolean }): Observable<void> =>
  combineLatest({
    cache: source.fetchData({ watch: watchData }),
    schemaDef: defer(async () => source.provideSchema()),
    targetPath: defer(makeArtifactsDir).pipe(
      tap((artifactsDir) => watchData && errorIfArtifactsDirIsDeleted({ artifactsDir })),
    ),
    generationOptions: of({ sourcePluginType: source.type, options: source.options }),
    writtenFilesCache: of({}),
  }).pipe(switchMap(writeFilesForCache))

export const generateDotpkgSingle = ({
  source,
}: {
  source: SourcePlugin
}): T.Effect<OT.HasTracer, Error | MkdirError, void> =>
  pipe(
    generateDotpkgEff({ source, watchData: false }),
    T.tap(() => T.succeedWith(() => console.log('worked'))),
    // S.take(1),
    // S.runCollect,
    // S.runDrain,
    // T.map(Chunk.unsafeHead),
    // OT.withSpan('@contentlayer/core/generation:generateDotpkgSingle', { attributes: {} }),
  )

export const generateDotpkgEff = ({ source, watchData }: { source: SourcePlugin; watchData: boolean }) => {
  // }): S.Stream<OT.HasTracer, Error | MkdirError, void> => {
  // const writtenFilesCache = {}
  // const generationOptions = { sourcePluginType: source.type, options: source.options }
  // const resolveParams = pipe(
  //   T.structPar({
  //     schemaDef: source.provideSchemaEff!,
  //     targetPath: makeArtifactsDirEff,
  //     // __: T.fail(new Error('oh no')),
  //     // .pipe(
  //     //   tap((artifactsDir) => watchData && errorIfArtifactsDirIsDeleted({ artifactsDir }))
  //     // ),
  //   }),
  // )

  const dataStream = source.fetchDataEff!({ watch: true })

  console.log({ dataStream })

  const drainedStream_ = dataStream as unknown as T.Effect<any, never, Cache>

  return pipe(
    drainedStream_,
    // dataStream,
    // S.take(1),
    // S.runDrain,
  )
}

const writeFilesForCacheEff = (params: {
  schemaDef: SchemaDef
  cache: Cache
  targetPath: string
  generationOptions: GenerationOptions
  writtenFilesCache: WrittenFilesCache
}): T.Effect<unknown, Error, void> =>
  T.tryCatchPromise(
    () => writeFilesForCache(params),
    (e) => e as Error,
  )

const writeFilesForCache = (async ({
  cache,
  schemaDef,
  targetPath,
  generationOptions,
  writtenFilesCache,
}: {
  schemaDef: SchemaDef
  cache: Cache
  targetPath: string
  generationOptions: GenerationOptions
  writtenFilesCache: WrittenFilesCache
}): Promise<void> => {
  console.log('write files')

  const withPrefix = (...path_: string[]) => path.join(targetPath, ...path_)

  if (process.env['CL_DEBUG']) {
    // NOTE cache directory already exists because `source.fetchData` has already created it
    await fs.writeFile(withPrefix('cache', 'schema.json'), JSON.stringify(schemaDef, null, 2))
  }

  const allCacheItems = Object.values(cache.cacheItemsMap)
  const allDocuments = allCacheItems.map((_) => _.document)

  const documentDefs = Object.values(schemaDef.documentTypeDefMap)

  const typeNameField = generationOptions.options.fieldOptions.typeFieldName
  const dataBarrelFiles = documentDefs.map((docDef) => ({
    content: makeDataExportFile({
      docDef,
      documentIds: allDocuments.filter((_) => _[typeNameField] === docDef.name).map((_) => _._id),
    }),
    filePath: withPrefix('data', `${getDataVariableName({ docDef })}.js`),
  }))

  const dataJsonFiles = allCacheItems.map(({ document, documentHash }) => ({
    content: JSON.stringify(document, null, 2),
    filePath: withPrefix('data', document[typeNameField], `${idToFileName(document._id)}.json`),
    documentHash,
  }))

  const dataDirPaths = documentDefs.map((_) => withPrefix('data', _.name))
  await Promise.all([mkdir(withPrefix('types')), ...dataDirPaths.map(mkdir)])

  const writeFile = writeFileWithWrittenFilesCache({ writtenFilesCache })

  await Promise.all([
    writeFile({ filePath: withPrefix('package.json'), content: makePackageJson() }),
    writeFile({
      filePath: withPrefix('types', 'index.d.ts'),
      content: renderTypes({ schemaDef, generationOptions }),
    }),
    writeFile({ filePath: withPrefix('types', 'index.js'), content: makeHelperTypes() }),
    writeFile({ filePath: withPrefix('data', 'index.d.ts'), content: makeDataTypes({ schemaDef }) }),
    writeFile({ filePath: withPrefix('data', 'index.js'), content: makeIndexJs({ schemaDef }) }),
    ...dataBarrelFiles.map(writeFile),
    ...dataJsonFiles.map(writeFile),
  ])
})['|>'](
  utils.traceAsyncFn('@contentlayer/core/commands/generate-dotpkg:writeFilesForCache', (_) => utils.omit(_, ['cache'])),
)

const makePackageJson = (): string => {
  const packageJson: PackageJson & { typesVersions: any } = {
    name: 'dot-contentlayer',
    description: 'This package is auto-generated by Contentlayer',
    version: '0.0.0',
    exports: {
      './data': {
        import: './data/index.js',
      },
      './types': {
        import: './types/index.js',
      },
    },
    typesVersions: {
      '*': {
        data: ['./data'],
        types: ['./types'],
      },
    },
  }

  return JSON.stringify(packageJson, null, 2)
}

const mkdir = async (dirPath: string) => {
  try {
    await fs.mkdir(dirPath, { recursive: true })
  } catch (e: any) {
    if (e.code !== 'EEXIST') {
      throw e
    }
  }
}

/**
 * Remembers which files already have been written to disk.
 * If no `documentHash` was provided, the writes won't be cached. */
const writeFileWithWrittenFilesCache =
  ({ writtenFilesCache }: { writtenFilesCache: WrittenFilesCache }) =>
  async ({
    filePath,
    content,
    documentHash,
  }: {
    filePath: string
    content: string
    documentHash?: string
  }): Promise<void> => {
    if (documentHash !== undefined && writtenFilesCache[filePath] === documentHash) {
      return
    }

    await fs.writeFile(filePath, content, 'utf8')
    if (documentHash) {
      writtenFilesCache[filePath] = documentHash
    }
  }

const makeDataExportFile = ({ docDef, documentIds }: { docDef: DocumentTypeDef; documentIds: string[] }): string => {
  const dataVariableName = getDataVariableName({ docDef })

  if (docDef.isSingleton) {
    const documentId = documentIds[0]
    return `\
// ${autogeneratedNote}
export { default as ${dataVariableName} } from './${docDef.name}/${idToFileName(documentId)}.json'
`
  }

  const makeVariableName = utils.flow(idToFileName, (_) => camelCase(_, { stripRegexp: /[^A-Z0-9\_]/gi }))

  const docImports = documentIds
    .map((_) => `import ${makeVariableName(_)} from './${docDef.name}/${idToFileName(_)}.json'`)
    .join('\n')

  return `\
// ${autogeneratedNote}

${docImports}

export const ${dataVariableName} = [${documentIds.map((_) => makeVariableName(_)).join(', ')}]
`
}

const makeIndexJs = ({ schemaDef }: { schemaDef: SchemaDef }): string => {
  const dataVariableNames = Object.values(schemaDef.documentTypeDefMap).map(
    (docDef) => [docDef, getDataVariableName({ docDef })] as const,
  )
  const constReexports = dataVariableNames
    .map(([, dataVariableName]) => `export * from './${dataVariableName}.js'`)
    .join('\n')

  const constImportsForAllDocuments = dataVariableNames
    .map(([, dataVariableName]) => `import { ${dataVariableName} } from './${dataVariableName}.js'`)
    .join('\n')

  const allDocuments = dataVariableNames
    .map(([docDef, dataVariableName]) => (docDef.isSingleton ? dataVariableName : `...${dataVariableName}`))
    .join(', ')

  return `\
// ${autogeneratedNote}

export { isType } from 'contentlayer/client'

${constReexports}
${constImportsForAllDocuments}

export const allDocuments = [${allDocuments}]
`
}

const makeHelperTypes = (): string => {
  return `\
// ${autogeneratedNote}

export { isType } from 'contentlayer/client'
`
}

const makeDataTypes = ({ schemaDef }: { schemaDef: SchemaDef }): string => {
  const dataConsts = Object.values(schemaDef.documentTypeDefMap)
    .map((docDef) => [docDef, docDef.name, getDataVariableName({ docDef })] as const)
    .map(
      ([docDef, typeName, dataVariableName]) =>
        `export declare const ${dataVariableName}: ${typeName}${docDef.isSingleton ? '' : '[]'}`,
    )
    .join('\n')

  const documentTypeNames = Object.values(schemaDef.documentTypeDefMap)
    .map((docDef) => docDef.name)
    .join(', ')

  return `\
// ${autogeneratedNote}

import { ${documentTypeNames}, DocumentTypes } from '../types'

${dataConsts}

export declare const allDocuments: DocumentTypes[]

`
}

const getDataVariableName = ({ docDef }: { docDef: DocumentTypeDef }): string => {
  if (docDef.isSingleton) {
    return utils.lowercaseFirstChar(utils.inflection.singularize(docDef.name))
  } else {
    return 'all' + utils.uppercaseFirstChar(utils.inflection.pluralize(docDef.name))
  }
}

const idToFileName = (id: string): string => {
  return leftPadWithUnderscoreIfStartsWithNumber(id).replace(/\//g, '__')
}

const leftPadWithUnderscoreIfStartsWithNumber = (str: string): string => {
  if (/^[0-9]/.test(str)) {
    return '_' + str
  }
  return str
}

const errorIfArtifactsDirIsDeleted = ({ artifactsDir }: { artifactsDir: string }) => {
  watch(artifactsDir, async (event) => {
    if (event === 'rename' && !(await fileOrDirExists(artifactsDir))) {
      console.error(`Seems like the target directory (${artifactsDir}) was deleted. Please restart the command.`)
      process.exit(1)
    }
  })
}
